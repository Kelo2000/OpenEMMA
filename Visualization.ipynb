{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743641b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os.path\n",
    "import re\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from math import atan2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from nuscenes import NuScenes\n",
    "from pyquaternion import Quaternion\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "\n",
    "import json\n",
    "from openemma.YOLO3D.inference import yolo3d_nuScenes\n",
    "from utils import EstimateCurvatureFromTrajectory, IntegrateCurvatureForPoints, OverlayTrajectory, WriteImageSequenceToVideo\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, Qwen2VLForConditionalGeneration, AutoTokenizer\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_PLACEHOLDER\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n",
    "from llava.conversation import conv_templates\n",
    "\n",
    "client = OpenAI(api_key=\"[your-openai-api-key]\")\n",
    "\n",
    "OBS_LEN = 10\n",
    "FUT_LEN = 10\n",
    "TTL_LEN = OBS_LEN + FUT_LEN\n",
    "from nuscenes.utils.geometry_utils import view_points, BoxVisibility\n",
    "from nuscenes.utils.data_classes import Box\n",
    "\n",
    "def draw_gt_boxes_and_labels(nusc, cam_token, img, line_thickness=1, text_scale=0.4):\n",
    "    \"\"\"\n",
    "    Draw GT 3D boxes (projected to 2D) and category labels on img (BGR).\n",
    "    cam_token: CAM_FRONT sample_data token.\n",
    "    img: np.ndarray (H,W,3) BGR (modified in place).\n",
    "    \"\"\"\n",
    "    # Get boxes already transformed into camera frame\n",
    "    data_path, boxes, cam_intrinsic = nusc.get_sample_data(\n",
    "        cam_token,\n",
    "        box_vis_level=BoxVisibility.ANY\n",
    "    )\n",
    "\n",
    "    for box in boxes:\n",
    "        # Project 3D corners to image plane\n",
    "        corners_3d = box.corners()  # (3, 8)\n",
    "        corners_2d = view_points(corners_3d, np.array(cam_intrinsic), normalize=True)  # (3, 8)\n",
    "        corners_2d = corners_2d[:2, :].T  # (8, 2)\n",
    "\n",
    "        # Convert to int and filter points inside image bounds a bit\n",
    "        corners_2d = corners_2d.astype(int)\n",
    "\n",
    "        # Define 3D box edges (pairs of corner indices)\n",
    "        edges = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 0),   # bottom face\n",
    "            (4, 5), (5, 6), (6, 7), (7, 4),   # top face\n",
    "            (0, 4), (1, 5), (2, 6), (3, 7)    # verticals\n",
    "        ]\n",
    "\n",
    "        # Draw thin lines\n",
    "        for a, b in edges:\n",
    "            x1, y1 = corners_2d[a]\n",
    "            x2, y2 = corners_2d[b]\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), thickness=line_thickness)\n",
    "\n",
    "        # Draw label (category name) near the top-front corner\n",
    "        label = box.name  # category string, e.g., 'vehicle.car'\n",
    "        # pick a corner with min y (closest to top of image)\n",
    "        top_idx = np.argmin(corners_2d[:, 1])\n",
    "        x_text, y_text = corners_2d[top_idx]\n",
    "\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (int(x_text), int(y_text) - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            text_scale,\n",
    "            (0, 255, 0),\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def getMessage(prompt, image=None, args=None):\n",
    "    if \"llama\" in args.model_path or \"Llama\" in args.model_path:\n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]}\n",
    "        ]\n",
    "    elif \"qwen\" in args.model_path or \"Qwen\" in args.model_path:\n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]}\n",
    "        ]   \n",
    "    return message\n",
    "\n",
    "\n",
    "def vlm_inference(text=None, images=None, sys_message=None, processor=None, model=None, tokenizer=None, args=None):\n",
    "        if \"llama\" in args.model_path or \"Llama\" in args.model_path:\n",
    "            image = Image.open(images).convert('RGB')\n",
    "            message = getMessage(text, args=args)\n",
    "            input_text = processor.apply_chat_template(message, add_generation_prompt=True)\n",
    "            inputs = processor(\n",
    "                image,\n",
    "                input_text,\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            output = model.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "            output_text = processor.decode(output[0])\n",
    "\n",
    "            if \"llama\" in args.model_path or \"Llama\" in args.model_path:\n",
    "                output_text = re.findall(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', output_text, re.DOTALL)[0].strip()\n",
    "            return output_text\n",
    "        \n",
    "        elif \"qwen\" in args.model_path or \"Qwen\" in args.model_path:\n",
    "            message = getMessage(text, image=images, args=args)\n",
    "            text = processor.apply_chat_template(\n",
    "                message, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(message)\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            return output_text[0]\n",
    "\n",
    "        elif \"llava\" in args.model_path:\n",
    "            conv_mode = \"mistral_instruct\"\n",
    "            image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "            if IMAGE_PLACEHOLDER in text:\n",
    "                if model.config.mm_use_im_start_end:\n",
    "                    text = re.sub(IMAGE_PLACEHOLDER, image_token_se, text)\n",
    "                else:\n",
    "                    text = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, text)\n",
    "            else:\n",
    "                if model.config.mm_use_im_start_end:\n",
    "                    text = image_token_se + \"\\n\" + text\n",
    "                else:\n",
    "                    text = DEFAULT_IMAGE_TOKEN + \"\\n\" + text\n",
    "\n",
    "            conv = conv_templates[conv_mode].copy()\n",
    "            conv.append_message(conv.roles[0], text)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "\n",
    "            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "            image = Image.open(images).convert('RGB')\n",
    "\n",
    "            image_tensor = process_images([image], processor, model.config)[0]\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "                    image_sizes=[image.size],\n",
    "                    do_sample=True,\n",
    "                    temperature=0.2,\n",
    "                    top_p=None,\n",
    "                    num_beams=1,\n",
    "                    max_new_tokens=2048,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id = tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "            return outputs\n",
    "                    \n",
    "        elif \"gpt\" in args.model_path:\n",
    "            PROMPT_MESSAGES = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        *map(lambda x: {\"image\": x, \"resize\": 768}, images),\n",
    "                        text,\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "            if sys_message is not None:\n",
    "                sys_message_dict = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": sys_message\n",
    "                }\n",
    "                PROMPT_MESSAGES.append(sys_message_dict)\n",
    "            params = {\n",
    "                \"model\": \"gpt-4o-2024-11-20\",\n",
    "                \"messages\": PROMPT_MESSAGES,\n",
    "                \"max_tokens\": 400,\n",
    "            }\n",
    "\n",
    "            result = client.chat.completions.create(**params)\n",
    "\n",
    "            return result.choices[0].message.content\n",
    "\n",
    "def SceneDescription(obs_images, processor=None, model=None, tokenizer=None, args=None):\n",
    "    prompt = f\"\"\"You are a autonomous driving labeller. You have access to these front-view camera images of a car taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Describe the driving scene according to traffic lights, movements of other cars or pedestrians and lane markings.\"\"\"\n",
    "\n",
    "    if \"llava\" in args.model_path:\n",
    "        prompt = f\"\"\"You are an autonomous driving labeller. You have access to these front-view camera images of a car taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Provide a concise description of the driving scene according to traffic lights, movements of other cars or pedestrians and lane markings.\"\"\"\n",
    "\n",
    "    result = vlm_inference(text=prompt, images=obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "    return result\n",
    "\n",
    "def DescribeObjects(obs_images, processor=None, model=None, tokenizer=None, args=None):\n",
    "\n",
    "    prompt = f\"\"\"You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. What other road users should you pay attention to in the driving scene? List two or three of them, specifying its location within the image of the driving scene and provide a short description of the that road user on what it is doing, and why it is important to you.\"\"\"\n",
    "\n",
    "    result = vlm_inference(text=prompt, images=obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "\n",
    "    return result\n",
    "\n",
    "def DescribeOrUpdateIntent(obs_images, prev_intent=None, processor=None, model=None, tokenizer=None, args=None):\n",
    "\n",
    "    if prev_intent is None:\n",
    "        prompt = f\"\"\"You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Based on the lane markings and the movement of other cars and pedestrians, describe the desired intent of the ego car. Is it going to follow the lane to turn left, turn right, or go straight? Should it maintain the current speed or slow down or speed up?\"\"\"\n",
    "\n",
    "        if \"llava\" in args.model_path:\n",
    "            prompt = f\"\"\"You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Based on the lane markings and the movement of other cars and pedestrians, provide a concise description of the desired intent of  the ego car. Is it going to follow the lane to turn left, turn right, or go straight? Should it maintain the current speed or slow down or speed up?\"\"\"\n",
    "        \n",
    "    else:\n",
    "        prompt = f\"\"\"You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Half a second ago your intent was to {prev_intent}. Based on the updated lane markings and the updated movement of other cars and pedestrians, do you keep your intent or do you change it? Explain your current intent: \"\"\"\n",
    "\n",
    "        if \"llava\" in args.model_path:\n",
    "            prompt = f\"\"\"You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle taken at a 0.5 second interval over the past 5 seconds. Imagine you are driving the car. Half a second ago your intent was to {prev_intent}. Based on the updated lane markings and the updated movement of other cars and pedestrians, do you keep your intent or do you change it? Provide a concise description explanation of your current intent: \"\"\"\n",
    "\n",
    "    result = vlm_inference(text=prompt, images=obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def GenerateMotion(obs_images, obs_waypoints, obs_velocities, obs_curvatures, given_intent, processor=None, model=None, tokenizer=None, args=None):\n",
    "    # assert len(obs_images) == len(obs_waypoints)\n",
    "\n",
    "    scene_description, object_description, intent_description = None, None, None\n",
    "\n",
    "    if args.method == \"openemma\":\n",
    "        scene_description = SceneDescription(obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "        object_description = DescribeObjects(obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "        intent_description = DescribeOrUpdateIntent(obs_images, prev_intent=given_intent, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "        print(f'Scene Description: {scene_description}')\n",
    "        print(f'Object Description: {object_description}')\n",
    "        print(f'Intent Description: {intent_description}')\n",
    "\n",
    "    # Convert array waypoints to string.\n",
    "    obs_waypoints_str = [f\"[{x[0]:.2f},{x[1]:.2f}]\" for x in obs_waypoints]\n",
    "    obs_waypoints_str = \", \".join(obs_waypoints_str)\n",
    "    obs_velocities_norm = np.linalg.norm(obs_velocities, axis=1)\n",
    "    obs_curvatures = obs_curvatures * 100\n",
    "    obs_speed_curvature_str = [f\"[{x[0]:.1f},{x[1]:.1f}]\" for x in zip(obs_velocities_norm, obs_curvatures)]\n",
    "    obs_speed_curvature_str = \", \".join(obs_speed_curvature_str)\n",
    "\n",
    "    \n",
    "    print(f'Observed Speed and Curvature: {obs_speed_curvature_str}')\n",
    "\n",
    "    sys_message = (\"You are a autonomous driving labeller. You have access to a front-view camera image of a vehicle, a sequence of past speeds, a sequence of past curvatures, and a driving rationale. Each speed, curvature is represented as [v, k], where v corresponds to the speed, and k corresponds to the curvature. A positive k means the vehicle is turning left. A negative k means the vehicle is turning right. The larger the absolute value of k, the sharper the turn. A close to zero k means the vehicle is driving straight. As a driver on the road, you should follow any common sense traffic rules. You should try to stay in the middle of your lane. You should maintain necessary distance from the leading vehicle. You should observe lane markings and follow them.  Your task is to do your best to predict future speeds and curvatures for the vehicle over the next 10 timesteps given vehicle intent inferred from the image. Make a best guess if the problem is too difficult for you. If you cannot provide a response people will get injured.\\n\")\n",
    "\n",
    "    if args.method == \"openemma\":\n",
    "        prompt = f\"\"\"These are frames from a video taken by a camera mounted in the front of a car. The images are taken at a 0.5 second interval. \n",
    "        The scene is described as follows: {scene_description}. \n",
    "        The identified critical objects are {object_description}. \n",
    "        The car's intent is {intent_description}. \n",
    "        The 5 second historical velocities and curvatures of the ego car are {obs_speed_curvature_str}. \n",
    "        Infer the association between these numbers and the image sequence. Generate the predicted future speeds and curvatures in the format [speed_1, curvature_1], [speed_2, curvature_2],..., [speed_10, curvature_10]. Write the raw text not markdown or latex. Future speeds and curvatures:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"These are frames from a video taken by a camera mounted in the front of a car. The images are taken at a 0.5 second interval. \n",
    "        The 5 second historical velocities and curvatures of the ego car are {obs_speed_curvature_str}. \n",
    "        Infer the association between these numbers and the image sequence. Generate the predicted future speeds and curvatures in the format [speed_1, curvature_1], [speed_2, curvature_2],..., [speed_10, curvature_10]. Write the raw text not markdown or latex. Future speeds and curvatures:\"\"\"\n",
    "    for rho in range(3):\n",
    "        result = vlm_inference(text=prompt, images=obs_images, sys_message=sys_message, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "        if not \"unable\" in result and not \"sorry\" in result and \"[\" in result:\n",
    "            break\n",
    "    return result, scene_description, object_description, intent_description\n",
    "\n",
    "\n",
    "\n",
    "def vlm_inference(text=None, images=None, sys_message=None, processor=None, model=None, tokenizer=None, args=None):\n",
    "    if (\"qwen\" in args.model_path or \"Qwen\" in args.model_path):\n",
    "        # 判断是否为Qwen2.5-VL-3B-Instruct（新版）\n",
    "        if hasattr(model, \"model_type\") and getattr(model, \"model_type\", \"\") == \"qwen2_5_vl\":\n",
    "            # Qwen2.5-VL-3B-Instruct官方推荐推理方式\n",
    "            message = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": images},\n",
    "                        {\"type\": \"text\", \"text\": text}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            text_prompt = processor.apply_chat_template(\n",
    "                message, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(message)\n",
    "            inputs = processor(\n",
    "                text=[text_prompt],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(model.device)\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            return output_text[0]\n",
    "        else:\n",
    "            # 兼容Qwen2-VL-7B-Instruct等老模型\n",
    "            message = getMessage(text, image=images, args=args)\n",
    "            text_prompt = processor.apply_chat_template(\n",
    "                message, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(message)\n",
    "            inputs = processor(\n",
    "                text=[text_prompt],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            return output_text[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4ad22",
   "metadata": {},
   "source": [
    "## Visualizing the Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--model-path\", type=str, default=\"gpt\")\n",
    "#     parser.add_argument(\"--plot\", type=bool, default=True)\n",
    "#     parser.add_argument(\"--dataroot\", type=str, default='datasets/NuScenes')\n",
    "#     parser.add_argument(\"--version\", type=str, default='v1.0-mini')\n",
    "#     parser.add_argument(\"--method\", type=str, default='openemma')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # In a notebook, just set args directly instead of using argparse\n",
    "    args = SimpleNamespace(\n",
    "        model_path=\"qwen\",                 # or \"llava\", \"Qwen/...\", etc.\n",
    "        plot=True,                        # True/False\n",
    "        dataroot=\"/scratch/ltl2113/LightEMMA/v1.0-mini\",     # <-- update to your path\n",
    "        version=\"v1.0-mini\",\n",
    "        method=\"openemma\",                # or whatever method you use\n",
    "    )\n",
    "\n",
    "    print(f\"{args.model_path}\")\n",
    "\n",
    "    model = None\n",
    "    processor = None\n",
    "    tokenizer = None\n",
    "    qwen25_loaded = False\n",
    "#     try:\n",
    "#         # 优先本地加载Qwen2.5-VL-3B-Instruct，并优选flash attention\n",
    "#         if \"qwen\" in args.model_path or \"Qwen\" in args.model_path:\n",
    "#             try:\n",
    "#                 print(\"Qwen2.5-VL-3B-Instruct 加载失败，尝试加载 Qwen2-VL-7B-Instruct。\")\n",
    "#                 print(e)\n",
    "#                 model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#                     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#                     torch_dtype=torch.bfloat16,\n",
    "#                     device_map=\"auto\"\n",
    "#                 )\n",
    "#                 processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "#                 tokenizer = None\n",
    "#                 qwen25_loaded = False\n",
    "#                 print(\"已加载 Qwen2-VL-7B-Instruct。\")\n",
    "#             except Exception as e:\n",
    "#                 print(\"Qwen2.5-VL-3B-Instruct 加载失败，尝试加载 Qwen2-VL-7B-Instruct。\")\n",
    "#                 print(e)\n",
    "#                 model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#                     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#                     torch_dtype=torch.bfloat16,\n",
    "#                     device_map=\"auto\"\n",
    "#                 )\n",
    "#                 processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "#                 tokenizer = None\n",
    "#                 qwen25_loaded = False\n",
    "#                 print(\"已加载 Qwen2-VL-7B-Instruct。\")\n",
    "#         else:\n",
    "#             if \"llava\" == args.model_path:    \n",
    "#                 disable_torch_init()\n",
    "#                 tokenizer, model, processor, context_len = load_pretrained_model(\"liuhaotian/llava-v1.6-mistral-7b\", None, \"llava-v1.6-mistral-7b\")\n",
    "#                 image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "#             elif \"llava\" in args.model_path:\n",
    "#                 disable_torch_init()\n",
    "#                 tokenizer, model, processor, context_len = load_pretrained_model(args.model_path, None, \"llava-v1.6-mistral-7b\")\n",
    "#                 image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "#             else:\n",
    "#                 model = None\n",
    "#                 processor = None\n",
    "#                 tokenizer=None\n",
    "#     except Exception as e:\n",
    "#         print(\"模型加载出现异常：\", e)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestamp = args.model_path + f\"_results/{args.method}/\" + timestamp\n",
    "    os.makedirs(timestamp, exist_ok=True)\n",
    "\n",
    "    # Load the dataset\n",
    "    nusc = NuScenes(version=args.version, dataroot=args.dataroot)\n",
    "\n",
    "    # Iterate the scenes\n",
    "    scenes = nusc.scene\n",
    "    \n",
    "    print(f\"Number of scenes: {len(scenes)}\")\n",
    "\n",
    "    for scene in scenes:\n",
    "        token = scene['token']\n",
    "        first_sample_token = scene['first_sample_token']\n",
    "        last_sample_token = scene['last_sample_token']\n",
    "        name = scene['name']\n",
    "        description = scene['description']\n",
    "\n",
    "#         if not name in [\"scene-0103\", \"scene-1077\"]:\n",
    "#             continue\n",
    "\n",
    "        # Get all image and pose in this scene\n",
    "        front_camera_images = []\n",
    "        ego_poses = []\n",
    "        camera_params = []\n",
    "        curr_sample_token = first_sample_token\n",
    "\n",
    "        # NEW: track sample tokens and annotations for this scene\n",
    "        sample_tokens = []              # index -> sample_token\n",
    "        sample_annotations = {}         # sample_token -> list of ann dicts\n",
    "\n",
    "        cam_front_tokens = []\n",
    "\n",
    "        while True:\n",
    "            sample = nusc.get('sample', curr_sample_token)\n",
    "\n",
    "            # Get the front camera image of the sample.\n",
    "            cam_front_data = nusc.get('sample_data', sample['data']['CAM_FRONT'])\n",
    "            cam_front_tokens.append(cam_front_data['token'])\n",
    "            # nusc.render_sample_data(cam_front_data['token'])\n",
    "\n",
    "            # NEW: store sample token in order\n",
    "            sample_tokens.append(curr_sample_token)\n",
    "\n",
    "            if \"gpt\" in args.model_path:\n",
    "                with open(os.path.join(nusc.dataroot, cam_front_data['filename']), \"rb\") as image_file:\n",
    "                    front_camera_images.append(base64.b64encode(image_file.read()).decode('utf-8'))\n",
    "            else:\n",
    "                front_camera_images.append(os.path.join(nusc.dataroot, cam_front_data['filename']))\n",
    "\n",
    "            # Get the ego pose of the sample.\n",
    "            pose = nusc.get('ego_pose', cam_front_data['ego_pose_token'])\n",
    "            ego_poses.append(pose)\n",
    "\n",
    "            # Get the camera parameters of the sample.\n",
    "            camera_params.append(nusc.get('calibrated_sensor', cam_front_data['calibrated_sensor_token']))\n",
    "\n",
    "             # NEW: collect per-object annotations for this sample\n",
    "            ann_list = []\n",
    "            for ann_token in sample['anns']:\n",
    "                ann = nusc.get('sample_annotation', ann_token)\n",
    "                visibility = nusc.get('visibility', ann['visibility_token'])['description']\n",
    "                ann_list.append({\n",
    "                    \"ann_token\": ann_token,\n",
    "                    \"category_name\": ann[\"category_name\"],\n",
    "                    \"translation\": ann[\"translation\"],   # [x, y, z]\n",
    "                    \"size\": ann[\"size\"],                 # [w, l, h]\n",
    "                    \"rotation\": ann[\"rotation\"],         # quaternion [w, x, y, z]\n",
    "                    \"visibility\": visibility,\n",
    "                    \"num_lidar_pts\": ann.get(\"num_lidar_pts\", None),\n",
    "                    \"num_radar_pts\": ann.get(\"num_radar_pts\", None),\n",
    "                })\n",
    "            sample_annotations[curr_sample_token] = ann_list\n",
    "            # END NEW block for annotations\n",
    "\n",
    "            # Advance the pointer.\n",
    "            if curr_sample_token == last_sample_token:\n",
    "                break\n",
    "            curr_sample_token = sample['next']\n",
    "\n",
    "        scene_length = len(front_camera_images)\n",
    "        print(f\"Scene {name} has {scene_length} frames\")\n",
    "\n",
    "       \n",
    "        if scene_length < TTL_LEN:\n",
    "            print(f\"Scene {name} has less than {TTL_LEN} frames, skipping...\")\n",
    "            continue\n",
    "\n",
    "        ## Compute interpolated trajectory.\n",
    "        # Get the velocities of the ego vehicle.\n",
    "        ego_poses_world = [ego_poses[t]['translation'][:3] for t in range(scene_length)]\n",
    "        ego_poses_world = np.array(ego_poses_world)\n",
    "        plt.plot(ego_poses_world[:, 0], ego_poses_world[:, 1], 'r-', label='GT')\n",
    "\n",
    "        ego_velocities = np.zeros_like(ego_poses_world)\n",
    "        ego_velocities[1:] = ego_poses_world[1:] - ego_poses_world[:-1]\n",
    "        ego_velocities[0] = ego_velocities[1]\n",
    "\n",
    "        # Get the curvature of the ego vehicle.\n",
    "        ego_curvatures = EstimateCurvatureFromTrajectory(ego_poses_world)\n",
    "        ego_velocities_norm = np.linalg.norm(ego_velocities, axis=1)\n",
    "        estimated_points = IntegrateCurvatureForPoints(ego_curvatures, ego_velocities_norm, ego_poses_world[0],\n",
    "                                                       atan2(ego_velocities[0][1], ego_velocities[0][0]), scene_length)\n",
    "\n",
    "        # Debug\n",
    "        if args.plot:\n",
    "            plt.quiver(ego_poses_world[:, 0], ego_poses_world[:, 1], ego_velocities[:, 0], ego_velocities[:, 1],\n",
    "                    color='b')\n",
    "            plt.plot(estimated_points[:, 0], estimated_points[:, 1], 'g-', label='Reconstruction')\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{timestamp}/{name}_interpolation.jpg\")\n",
    "            plt.close()\n",
    "\n",
    "        # Get the waypoints of the ego vehicle.\n",
    "        ego_traj_world = [ego_poses[t]['translation'][:3] for t in range(scene_length)]\n",
    "\n",
    "        prev_intent = None\n",
    "        cam_images_sequence = []\n",
    "        ade1s_list = []\n",
    "        ade2s_list = []\n",
    "        ade3s_list = []\n",
    "        TTL_LEN=20\n",
    "        for i in range(scene_length - TTL_LEN):\n",
    "            # Get the raw image data.\n",
    "            # utils.PlotBase64Image(front_camera_images[0])\n",
    "            obs_images = front_camera_images[i:i+OBS_LEN]\n",
    "            obs_ego_poses = ego_poses[i:i+OBS_LEN]\n",
    "            obs_camera_params = camera_params[i:i+OBS_LEN]\n",
    "            obs_ego_traj_world = ego_traj_world[i:i+OBS_LEN]\n",
    "            fut_ego_traj_world = ego_traj_world[i+OBS_LEN:i+TTL_LEN]\n",
    "            obs_ego_velocities = ego_velocities[i:i+OBS_LEN]\n",
    "            obs_ego_curvatures = ego_curvatures[i:i+OBS_LEN]\n",
    "\n",
    "            # Get positions of the vehicle.\n",
    "            obs_start_world = obs_ego_traj_world[0]\n",
    "            fut_start_world = obs_ego_traj_world[-1]\n",
    "            curr_image = obs_images[-1]\n",
    "\n",
    "            # obs_images = [curr_image]\n",
    "\n",
    "            # Allocate the images.\n",
    "            if \"gpt\" in args.model_path:\n",
    "                img = cv2.imdecode(np.frombuffer(base64.b64decode(curr_image), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "                img = yolo3d_nuScenes(img, calib=obs_camera_params[-1])[0]\n",
    "            else:\n",
    "                with open(os.path.join(curr_image), \"rb\") as image_file:\n",
    "                    img = cv2.imdecode(np.frombuffer(image_file.read(), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "#             for rho in range(3):\n",
    "#                 # Assemble the prompt.\n",
    "#                 if not \"gpt\" in args.model_path:\n",
    "#                     obs_images = curr_image\n",
    "#                 (prediction,\n",
    "#                 scene_description,\n",
    "#                 object_description,\n",
    "#                 updated_intent) = GenerateMotion(obs_images, obs_ego_traj_world, obs_ego_velocities,\n",
    "#                                                 obs_ego_curvatures, prev_intent, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "\n",
    "#                 # Process the output.\n",
    "#                 prev_intent = updated_intent  # Stateful intent\n",
    "#                 pred_waypoints = prediction.replace(\"Future speeds and curvatures:\", \"\").strip()\n",
    "#                 coordinates = re.findall(r\"\\[([-+]?\\d*\\.?\\d+),\\s*([-+]?\\d*\\.?\\d+)\\]\", pred_waypoints)\n",
    "#                 if not coordinates == []:\n",
    "#                     break\n",
    "#             if coordinates == []:\n",
    "#                 continue\n",
    "#             speed_curvature_pred = [[float(v), float(k)] for v, k in coordinates]\n",
    "#             speed_curvature_pred = speed_curvature_pred[:10]\n",
    "#             print(f\"Got {len(speed_curvature_pred)} future actions: {speed_curvature_pred}\")\n",
    "\n",
    "             # NEW: save one JSON per *current* sample/frame\n",
    "            # current frame index corresponding to curr_image\n",
    "            sample_idx = i + OBS_LEN - 1\n",
    "            current_sample_token = sample_tokens[sample_idx]\n",
    "            current_annots = sample_annotations.get(current_sample_token, [])\n",
    "\n",
    "#             sample_record = {\n",
    "#                 \"scene_name\": name,\n",
    "#                 # \"scene_token\": token,\n",
    "#                 \"scene_description_text\": description,\n",
    "#                 \"sample_index\": int(i),\n",
    "#                 # \"sample_token\": current_sample_token,\n",
    "#                 \"image_path\": curr_image,\n",
    "#                 \"model_path\": args.model_path,\n",
    "#                 # \"method\": args.method,\n",
    "#                 \"vlm_scene_description\": scene_description,\n",
    "#                 \"vlm_object_description\": object_description,\n",
    "#                 \"vlm_intent_description\": updated_intent,\n",
    "#                 # \"predicted_speed_curvature\": speed_curvature_pred,\n",
    "#                 \"annotations\": current_annots\n",
    "#             }\n",
    "\n",
    "        \n",
    "\n",
    "#             json_filename = f\"{name}_sample_{i}.json\"\n",
    "#             json_folder = os.path.join(timestamp,\"JsonFiles\")\n",
    "#             os.makedirs(json_folder, exist_ok=True)\n",
    "#             json_path = os.path.join(json_folder, json_filename)\n",
    "#             with open(json_path, \"w\") as jf:\n",
    "#                 json.dump(sample_record, jf, indent=2)\n",
    "\n",
    "#             gt_img_out = os.path.join(json_folder, f\"{name}_sample_{i}_gt_boxes.png\")\n",
    "            cam_token = cam_front_tokens[sample_idx]\n",
    "            # # nusc.render_sample_data(cam_token, out_path=gt_img_out, verbose=False)\n",
    "            # nusc.render_sample_data(\n",
    "            #     cam_token,\n",
    "            #     with_anns=True,     # <- make sure this is set\n",
    "            #     out_path=gt_img_out,\n",
    "            #     verbose=False\n",
    "            # )\n",
    "#             img = draw_gt_boxes_and_labels(nusc, cam_token, img, line_thickness=1, text_scale=0.4)\n",
    "            img = draw_gt_boxes_and_labels(nusc, cam_token, img, line_thickness=1, text_scale=0.4)\n",
    "#             cv2.imwrite(gt_img_out, img)\n",
    "            \n",
    "\n",
    "            # Convert BGR (OpenCV) -> RGB (matplotlib)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            plt.imshow(img_rgb)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{name} – sample {i} with GT boxes\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#             # END NEW: per-sample JSON dump\n",
    "\n",
    "#             # GT\n",
    "#             # OverlayTrajectory(img, fut_ego_traj_world, obs_camera_params[-1], obs_ego_poses[-1], color=(255, 0, 0))\n",
    "\n",
    "#             # Pred\n",
    "#             pred_len = min(FUT_LEN, len(speed_curvature_pred))\n",
    "#             pred_curvatures = np.array(speed_curvature_pred)[:, 1] / 100\n",
    "#             pred_speeds = np.array(speed_curvature_pred)[:, 0]\n",
    "#             pred_traj = np.zeros((pred_len, 3))\n",
    "#             pred_traj[:pred_len, :2] = IntegrateCurvatureForPoints(pred_curvatures,\n",
    "#                                                                    pred_speeds,\n",
    "#                                                                    fut_start_world,\n",
    "#                                                                    atan2(obs_ego_velocities[-1][1],\n",
    "#                                                                          obs_ego_velocities[-1][0]), pred_len)\n",
    "\n",
    "#             # Overlay the trajectory.\n",
    "#             check_flag = OverlayTrajectory(img, pred_traj.tolist(), obs_camera_params[-1], obs_ego_poses[-1], color=(255, 0, 0), args=args)\n",
    "            \n",
    "\n",
    "#             # Compute ADE.\n",
    "#             fut_ego_traj_world = np.array(fut_ego_traj_world)\n",
    "#             ade = np.mean(np.linalg.norm(fut_ego_traj_world[:pred_len] - pred_traj, axis=1))\n",
    "            \n",
    "#             pred1_len = min(pred_len, 2)\n",
    "#             ade1s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred1_len] - pred_traj[1:pred1_len+1] , axis=1))\n",
    "#             ade1s_list.append(ade1s)\n",
    "\n",
    "#             pred2_len = min(pred_len, 4)\n",
    "#             ade2s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred2_len] - pred_traj[:pred2_len] , axis=1))\n",
    "#             ade2s_list.append(ade2s)\n",
    "\n",
    "#             pred3_len = min(pred_len, 6)\n",
    "#             ade3s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred3_len] - pred_traj[:pred3_len] , axis=1))\n",
    "#             ade3s_list.append(ade3s)\n",
    "\n",
    "#             # Write to image.\n",
    "#             if args.plot == True:\n",
    "#                 cam_images_sequence.append(img.copy())\n",
    "#                 cv2.imwrite(f\"{timestamp}/{name}_{i}_front_cam.jpg\", img)\n",
    "\n",
    "#                 # Plot the trajectory.\n",
    "#                 plt.plot(fut_ego_traj_world[:, 0], fut_ego_traj_world[:, 1], 'r-', label='GT')\n",
    "#                 plt.plot(pred_traj[:, 0], pred_traj[:, 1], 'b-', label='Pred')\n",
    "#                 plt.legend()\n",
    "#                 plt.title(f\"Scene: {name}, Frame: {i}, ADE: {ade}\")\n",
    "#                 plt.savefig(f\"{timestamp}/{name}_{i}_traj.jpg\")\n",
    "#                 plt.close()\n",
    "\n",
    "#                 # Save the trajectory\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_traj.npy\", pred_traj)\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_curvatures.npy\", pred_curvatures)\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_speeds.npy\", pred_speeds)\n",
    "\n",
    "#                 # Save the descriptions\n",
    "#                 with open(f\"{timestamp}/{name}_{i}_logs.txt\", 'w') as f:\n",
    "#                     f.write(f\"Scene Description: {scene_description}\\n\")\n",
    "#                     f.write(f\"Object Description: {object_description}\\n\")\n",
    "#                     f.write(f\"Intent Description: {updated_intent}\\n\")\n",
    "#                     f.write(f\"Average Displacement Error: {ade}\\n\")\n",
    "\n",
    "#             # break  # Timestep\n",
    "\n",
    "#         mean_ade1s = np.mean(ade1s_list)\n",
    "#         mean_ade2s = np.mean(ade2s_list)\n",
    "#         mean_ade3s = np.mean(ade3s_list)\n",
    "#         aveg_ade = np.mean([mean_ade1s, mean_ade2s, mean_ade3s])\n",
    "\n",
    "#         result = {\n",
    "#             \"name\": name,\n",
    "#             \"token\": token,\n",
    "#             \"ade1s\": mean_ade1s,\n",
    "#             \"ade2s\": mean_ade2s,\n",
    "#             \"ade3s\": mean_ade3s,\n",
    "#             \"avgade\": aveg_ade\n",
    "#         }\n",
    "\n",
    "#         with open(f\"{timestamp}/ade_results.jsonl\", \"a\") as f:\n",
    "#             f.write(json.dumps(result))\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "#         if args.plot:\n",
    "#             WriteImageSequenceToVideo(cam_images_sequence, f\"{timestamp}/{name}\")\n",
    "\n",
    "#         # break  # Scenes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--model-path\", type=str, default=\"gpt\")\n",
    "#     parser.add_argument(\"--plot\", type=bool, default=True)\n",
    "#     parser.add_argument(\"--dataroot\", type=str, default='datasets/NuScenes')\n",
    "#     parser.add_argument(\"--version\", type=str, default='v1.0-mini')\n",
    "#     parser.add_argument(\"--method\", type=str, default='openemma')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # In a notebook, just set args directly instead of using argparse\n",
    "    args = SimpleNamespace(\n",
    "        model_path=\"qwen\",                 # or \"llava\", \"Qwen/...\", etc.\n",
    "        plot=True,                        # True/False\n",
    "        dataroot=\"/scratch/ltl2113/LightEMMA/v1.0-mini\",     # <-- update to your path\n",
    "        version=\"v1.0-mini\",\n",
    "        method=\"openemma\",                # or whatever method you use\n",
    "    )\n",
    "\n",
    "    print(f\"{args.model_path}\")\n",
    "\n",
    "    model = None\n",
    "    processor = None\n",
    "    tokenizer = None\n",
    "    qwen25_loaded = False\n",
    "#     try:\n",
    "#         # 优先本地加载Qwen2.5-VL-3B-Instruct，并优选flash attention\n",
    "#         if \"qwen\" in args.model_path or \"Qwen\" in args.model_path:\n",
    "#             try:\n",
    "#                 print(\"Qwen2.5-VL-3B-Instruct 加载失败，尝试加载 Qwen2-VL-7B-Instruct。\")\n",
    "#                 print(e)\n",
    "#                 model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#                     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#                     torch_dtype=torch.bfloat16,\n",
    "#                     device_map=\"auto\"\n",
    "#                 )\n",
    "#                 processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "#                 tokenizer = None\n",
    "#                 qwen25_loaded = False\n",
    "#                 print(\"已加载 Qwen2-VL-7B-Instruct。\")\n",
    "#             except Exception as e:\n",
    "#                 print(\"Qwen2.5-VL-3B-Instruct 加载失败，尝试加载 Qwen2-VL-7B-Instruct。\")\n",
    "#                 print(e)\n",
    "#                 model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#                     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#                     torch_dtype=torch.bfloat16,\n",
    "#                     device_map=\"auto\"\n",
    "#                 )\n",
    "#                 processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "#                 tokenizer = None\n",
    "#                 qwen25_loaded = False\n",
    "#                 print(\"已加载 Qwen2-VL-7B-Instruct。\")\n",
    "#         else:\n",
    "#             if \"llava\" == args.model_path:    \n",
    "#                 disable_torch_init()\n",
    "#                 tokenizer, model, processor, context_len = load_pretrained_model(\"liuhaotian/llava-v1.6-mistral-7b\", None, \"llava-v1.6-mistral-7b\")\n",
    "#                 image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "#             elif \"llava\" in args.model_path:\n",
    "#                 disable_torch_init()\n",
    "#                 tokenizer, model, processor, context_len = load_pretrained_model(args.model_path, None, \"llava-v1.6-mistral-7b\")\n",
    "#                 image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "#             else:\n",
    "#                 model = None\n",
    "#                 processor = None\n",
    "#                 tokenizer=None\n",
    "#     except Exception as e:\n",
    "#         print(\"模型加载出现异常：\", e)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestamp = args.model_path + f\"_results/{args.method}/\" + timestamp\n",
    "    os.makedirs(timestamp, exist_ok=True)\n",
    "\n",
    "    # Load the dataset\n",
    "    nusc = NuScenes(version=args.version, dataroot=args.dataroot)\n",
    "\n",
    "    # Iterate the scenes\n",
    "    scenes = nusc.scene\n",
    "    \n",
    "    print(f\"Number of scenes: {len(scenes)}\")\n",
    "\n",
    "    for scene in scenes:\n",
    "        token = scene['token']\n",
    "        first_sample_token = scene['first_sample_token']\n",
    "        last_sample_token = scene['last_sample_token']\n",
    "        name = scene['name']\n",
    "        description = scene['description']\n",
    "\n",
    "#         if not name in [\"scene-0103\", \"scene-1077\"]:\n",
    "#             continue\n",
    "\n",
    "        # Get all image and pose in this scene\n",
    "        front_camera_images = []\n",
    "        ego_poses = []\n",
    "        camera_params = []\n",
    "        curr_sample_token = first_sample_token\n",
    "\n",
    "        # NEW: track sample tokens and annotations for this scene\n",
    "        sample_tokens = []              # index -> sample_token\n",
    "        sample_annotations = {}         # sample_token -> list of ann dicts\n",
    "\n",
    "        cam_front_tokens = []\n",
    "\n",
    "        while True:\n",
    "            sample = nusc.get('sample', curr_sample_token)\n",
    "\n",
    "            # Get the front camera image of the sample.\n",
    "            cam_front_data = nusc.get('sample_data', sample['data']['CAM_FRONT'])\n",
    "            cam_front_tokens.append(cam_front_data['token'])\n",
    "            # nusc.render_sample_data(cam_front_data['token'])\n",
    "\n",
    "            # NEW: store sample token in order\n",
    "            sample_tokens.append(curr_sample_token)\n",
    "\n",
    "            if \"gpt\" in args.model_path:\n",
    "                with open(os.path.join(nusc.dataroot, cam_front_data['filename']), \"rb\") as image_file:\n",
    "                    front_camera_images.append(base64.b64encode(image_file.read()).decode('utf-8'))\n",
    "            else:\n",
    "                front_camera_images.append(os.path.join(nusc.dataroot, cam_front_data['filename']))\n",
    "\n",
    "            # Get the ego pose of the sample.\n",
    "            pose = nusc.get('ego_pose', cam_front_data['ego_pose_token'])\n",
    "            ego_poses.append(pose)\n",
    "\n",
    "            # Get the camera parameters of the sample.\n",
    "            camera_params.append(nusc.get('calibrated_sensor', cam_front_data['calibrated_sensor_token']))\n",
    "\n",
    "             # NEW: collect per-object annotations for this sample\n",
    "            ann_list = []\n",
    "            for ann_token in sample['anns']:\n",
    "                ann = nusc.get('sample_annotation', ann_token)\n",
    "                visibility = nusc.get('visibility', ann['visibility_token'])['description']\n",
    "                # only keep fully visible objects (NuScenes: token 4 is 100% visible)\n",
    "                vis_token = int(ann['visibility_token'])\n",
    "                if vis_token != 4:\n",
    "                    continue\n",
    "        \n",
    "                ann_list.append({\n",
    "                    \"ann_token\": ann_token,\n",
    "                    \"category_name\": ann[\"category_name\"],\n",
    "                    \"translation\": ann[\"translation\"],   # [x, y, z]\n",
    "                    \"size\": ann[\"size\"],                 # [w, l, h]\n",
    "                    \"rotation\": ann[\"rotation\"],         # quaternion [w, x, y, z]\n",
    "                    \"visibility\": visibility,\n",
    "                    \"num_lidar_pts\": ann.get(\"num_lidar_pts\", None),\n",
    "                    \"num_radar_pts\": ann.get(\"num_radar_pts\", None),\n",
    "                })\n",
    "            sample_annotations[curr_sample_token] = ann_list\n",
    "            # END NEW block for annotations\n",
    "\n",
    "            # Advance the pointer.\n",
    "            if curr_sample_token == last_sample_token:\n",
    "                break\n",
    "            curr_sample_token = sample['next']\n",
    "\n",
    "        scene_length = len(front_camera_images)\n",
    "        print(f\"Scene {name} has {scene_length} frames\")\n",
    "\n",
    "       \n",
    "        if scene_length < TTL_LEN:\n",
    "            print(f\"Scene {name} has less than {TTL_LEN} frames, skipping...\")\n",
    "            continue\n",
    "\n",
    "        ## Compute interpolated trajectory.\n",
    "        # Get the velocities of the ego vehicle.\n",
    "        ego_poses_world = [ego_poses[t]['translation'][:3] for t in range(scene_length)]\n",
    "        ego_poses_world = np.array(ego_poses_world)\n",
    "        plt.plot(ego_poses_world[:, 0], ego_poses_world[:, 1], 'r-', label='GT')\n",
    "\n",
    "        ego_velocities = np.zeros_like(ego_poses_world)\n",
    "        ego_velocities[1:] = ego_poses_world[1:] - ego_poses_world[:-1]\n",
    "        ego_velocities[0] = ego_velocities[1]\n",
    "\n",
    "        # Get the curvature of the ego vehicle.\n",
    "        ego_curvatures = EstimateCurvatureFromTrajectory(ego_poses_world)\n",
    "        ego_velocities_norm = np.linalg.norm(ego_velocities, axis=1)\n",
    "        estimated_points = IntegrateCurvatureForPoints(ego_curvatures, ego_velocities_norm, ego_poses_world[0],\n",
    "                                                       atan2(ego_velocities[0][1], ego_velocities[0][0]), scene_length)\n",
    "\n",
    "        # Debug\n",
    "        if args.plot:\n",
    "            plt.quiver(ego_poses_world[:, 0], ego_poses_world[:, 1], ego_velocities[:, 0], ego_velocities[:, 1],\n",
    "                    color='b')\n",
    "            plt.plot(estimated_points[:, 0], estimated_points[:, 1], 'g-', label='Reconstruction')\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{timestamp}/{name}_interpolation.jpg\")\n",
    "            plt.close()\n",
    "\n",
    "        # Get the waypoints of the ego vehicle.\n",
    "        ego_traj_world = [ego_poses[t]['translation'][:3] for t in range(scene_length)]\n",
    "\n",
    "        prev_intent = None\n",
    "        cam_images_sequence = []\n",
    "        ade1s_list = []\n",
    "        ade2s_list = []\n",
    "        ade3s_list = []\n",
    "        TTL_LEN=20\n",
    "        for i in range(scene_length - TTL_LEN):\n",
    "            # Get the raw image data.\n",
    "            # utils.PlotBase64Image(front_camera_images[0])\n",
    "            obs_images = front_camera_images[i:i+OBS_LEN]\n",
    "            obs_ego_poses = ego_poses[i:i+OBS_LEN]\n",
    "            obs_camera_params = camera_params[i:i+OBS_LEN]\n",
    "            obs_ego_traj_world = ego_traj_world[i:i+OBS_LEN]\n",
    "            fut_ego_traj_world = ego_traj_world[i+OBS_LEN:i+TTL_LEN]\n",
    "            obs_ego_velocities = ego_velocities[i:i+OBS_LEN]\n",
    "            obs_ego_curvatures = ego_curvatures[i:i+OBS_LEN]\n",
    "\n",
    "            # Get positions of the vehicle.\n",
    "            obs_start_world = obs_ego_traj_world[0]\n",
    "            fut_start_world = obs_ego_traj_world[-1]\n",
    "            curr_image = obs_images[-1]\n",
    "\n",
    "            # obs_images = [curr_image]\n",
    "\n",
    "            # Allocate the images.\n",
    "            if \"gpt\" in args.model_path:\n",
    "                img = cv2.imdecode(np.frombuffer(base64.b64decode(curr_image), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "                img = yolo3d_nuScenes(img, calib=obs_camera_params[-1])[0]\n",
    "            else:\n",
    "                with open(os.path.join(curr_image), \"rb\") as image_file:\n",
    "                    img = cv2.imdecode(np.frombuffer(image_file.read(), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "#             for rho in range(3):\n",
    "#                 # Assemble the prompt.\n",
    "#                 if not \"gpt\" in args.model_path:\n",
    "#                     obs_images = curr_image\n",
    "#                 (prediction,\n",
    "#                 scene_description,\n",
    "#                 object_description,\n",
    "#                 updated_intent) = GenerateMotion(obs_images, obs_ego_traj_world, obs_ego_velocities,\n",
    "#                                                 obs_ego_curvatures, prev_intent, processor=processor, model=model, tokenizer=tokenizer, args=args)\n",
    "\n",
    "#                 # Process the output.\n",
    "#                 prev_intent = updated_intent  # Stateful intent\n",
    "#                 pred_waypoints = prediction.replace(\"Future speeds and curvatures:\", \"\").strip()\n",
    "#                 coordinates = re.findall(r\"\\[([-+]?\\d*\\.?\\d+),\\s*([-+]?\\d*\\.?\\d+)\\]\", pred_waypoints)\n",
    "#                 if not coordinates == []:\n",
    "#                     break\n",
    "#             if coordinates == []:\n",
    "#                 continue\n",
    "#             speed_curvature_pred = [[float(v), float(k)] for v, k in coordinates]\n",
    "#             speed_curvature_pred = speed_curvature_pred[:10]\n",
    "#             print(f\"Got {len(speed_curvature_pred)} future actions: {speed_curvature_pred}\")\n",
    "\n",
    "             # NEW: save one JSON per *current* sample/frame\n",
    "            # current frame index corresponding to curr_image\n",
    "            sample_idx = i + OBS_LEN - 1\n",
    "            current_sample_token = sample_tokens[sample_idx]\n",
    "            current_annots = sample_annotations.get(current_sample_token, [])\n",
    "#             print(current_annots)\n",
    "#             sample_record = {\n",
    "#                 \"scene_name\": name,\n",
    "#                 # \"scene_token\": token,\n",
    "#                 \"scene_description_text\": description,\n",
    "#                 \"sample_index\": int(i),\n",
    "#                 # \"sample_token\": current_sample_token,\n",
    "#                 \"image_path\": curr_image,\n",
    "#                 \"model_path\": args.model_path,\n",
    "#                 # \"method\": args.method,\n",
    "#                 \"vlm_scene_description\": scene_description,\n",
    "#                 \"vlm_object_description\": object_description,\n",
    "#                 \"vlm_intent_description\": updated_intent,\n",
    "#                 # \"predicted_speed_curvature\": speed_curvature_pred,\n",
    "#                 \"annotations\": current_annots\n",
    "#             }\n",
    "\n",
    "        \n",
    "\n",
    "#             json_filename = f\"{name}_sample_{i}.json\"\n",
    "#             json_folder = os.path.join(timestamp,\"JsonFiles\")\n",
    "#             os.makedirs(json_folder, exist_ok=True)\n",
    "#             json_path = os.path.join(json_folder, json_filename)\n",
    "#             with open(json_path, \"w\") as jf:\n",
    "#                 json.dump(sample_record, jf, indent=2)\n",
    "\n",
    "#             gt_img_out = os.path.join(json_folder, f\"{name}_sample_{i}_gt_boxes.png\")\n",
    "            cam_token = cam_front_tokens[sample_idx]\n",
    "            # # nusc.render_sample_data(cam_token, out_path=gt_img_out, verbose=False)\n",
    "            # nusc.render_sample_data(\n",
    "            #     cam_token,\n",
    "            #     with_anns=True,     # <- make sure this is set\n",
    "            #     out_path=gt_img_out,\n",
    "            #     verbose=False\n",
    "            # )\n",
    "#             img = draw_gt_boxes_and_labels(nusc, cam_token, img, line_thickness=1, text_scale=0.4)\n",
    "            img = draw_gt_boxes_and_labels(nusc, cam_token, img, line_thickness=1, text_scale=0.4)\n",
    "#             cv2.imwrite(gt_img_out, img)\n",
    "            \n",
    "\n",
    "            # Convert BGR (OpenCV) -> RGB (matplotlib)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            plt.imshow(img_rgb)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{name} – sample {i} with GT boxes\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#             # END NEW: per-sample JSON dump\n",
    "\n",
    "#             # GT\n",
    "#             # OverlayTrajectory(img, fut_ego_traj_world, obs_camera_params[-1], obs_ego_poses[-1], color=(255, 0, 0))\n",
    "\n",
    "#             # Pred\n",
    "#             pred_len = min(FUT_LEN, len(speed_curvature_pred))\n",
    "#             pred_curvatures = np.array(speed_curvature_pred)[:, 1] / 100\n",
    "#             pred_speeds = np.array(speed_curvature_pred)[:, 0]\n",
    "#             pred_traj = np.zeros((pred_len, 3))\n",
    "#             pred_traj[:pred_len, :2] = IntegrateCurvatureForPoints(pred_curvatures,\n",
    "#                                                                    pred_speeds,\n",
    "#                                                                    fut_start_world,\n",
    "#                                                                    atan2(obs_ego_velocities[-1][1],\n",
    "#                                                                          obs_ego_velocities[-1][0]), pred_len)\n",
    "\n",
    "#             # Overlay the trajectory.\n",
    "#             check_flag = OverlayTrajectory(img, pred_traj.tolist(), obs_camera_params[-1], obs_ego_poses[-1], color=(255, 0, 0), args=args)\n",
    "            \n",
    "\n",
    "#             # Compute ADE.\n",
    "#             fut_ego_traj_world = np.array(fut_ego_traj_world)\n",
    "#             ade = np.mean(np.linalg.norm(fut_ego_traj_world[:pred_len] - pred_traj, axis=1))\n",
    "            \n",
    "#             pred1_len = min(pred_len, 2)\n",
    "#             ade1s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred1_len] - pred_traj[1:pred1_len+1] , axis=1))\n",
    "#             ade1s_list.append(ade1s)\n",
    "\n",
    "#             pred2_len = min(pred_len, 4)\n",
    "#             ade2s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred2_len] - pred_traj[:pred2_len] , axis=1))\n",
    "#             ade2s_list.append(ade2s)\n",
    "\n",
    "#             pred3_len = min(pred_len, 6)\n",
    "#             ade3s = np.mean(np.linalg.norm(fut_ego_traj_world[:pred3_len] - pred_traj[:pred3_len] , axis=1))\n",
    "#             ade3s_list.append(ade3s)\n",
    "\n",
    "#             # Write to image.\n",
    "#             if args.plot == True:\n",
    "#                 cam_images_sequence.append(img.copy())\n",
    "#                 cv2.imwrite(f\"{timestamp}/{name}_{i}_front_cam.jpg\", img)\n",
    "\n",
    "#                 # Plot the trajectory.\n",
    "#                 plt.plot(fut_ego_traj_world[:, 0], fut_ego_traj_world[:, 1], 'r-', label='GT')\n",
    "#                 plt.plot(pred_traj[:, 0], pred_traj[:, 1], 'b-', label='Pred')\n",
    "#                 plt.legend()\n",
    "#                 plt.title(f\"Scene: {name}, Frame: {i}, ADE: {ade}\")\n",
    "#                 plt.savefig(f\"{timestamp}/{name}_{i}_traj.jpg\")\n",
    "#                 plt.close()\n",
    "\n",
    "#                 # Save the trajectory\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_traj.npy\", pred_traj)\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_curvatures.npy\", pred_curvatures)\n",
    "#                 np.save(f\"{timestamp}/{name}_{i}_pred_speeds.npy\", pred_speeds)\n",
    "\n",
    "#                 # Save the descriptions\n",
    "#                 with open(f\"{timestamp}/{name}_{i}_logs.txt\", 'w') as f:\n",
    "#                     f.write(f\"Scene Description: {scene_description}\\n\")\n",
    "#                     f.write(f\"Object Description: {object_description}\\n\")\n",
    "#                     f.write(f\"Intent Description: {updated_intent}\\n\")\n",
    "#                     f.write(f\"Average Displacement Error: {ade}\\n\")\n",
    "\n",
    "#             # break  # Timestep\n",
    "\n",
    "#         mean_ade1s = np.mean(ade1s_list)\n",
    "#         mean_ade2s = np.mean(ade2s_list)\n",
    "#         mean_ade3s = np.mean(ade3s_list)\n",
    "#         aveg_ade = np.mean([mean_ade1s, mean_ade2s, mean_ade3s])\n",
    "\n",
    "#         result = {\n",
    "#             \"name\": name,\n",
    "#             \"token\": token,\n",
    "#             \"ade1s\": mean_ade1s,\n",
    "#             \"ade2s\": mean_ade2s,\n",
    "#             \"ade3s\": mean_ade3s,\n",
    "#             \"avgade\": aveg_ade\n",
    "#         }\n",
    "\n",
    "#         with open(f\"{timestamp}/ade_results.jsonl\", \"a\") as f:\n",
    "#             f.write(json.dumps(result))\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "#         if args.plot:\n",
    "#             WriteImageSequenceToVideo(cam_images_sequence, f\"{timestamp}/{name}\")\n",
    "\n",
    "#         # break  # Scenes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964c848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openemma",
   "language": "python",
   "name": "openemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
