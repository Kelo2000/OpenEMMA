{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c22ea6-1509-40cf-af45-5027d66296fe",
   "metadata": {},
   "source": [
    "### Setting cache to eb within our project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551dbce8-27e6-43c3-8b14-02babbc45aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!export HF_HOME=\"/home/jovyan/groupshare/lukelo_duoaa_project\"\n",
    "!export PYTHONPYCACHEPREFIX=\"/home/jovyan/groupshare/lukelo_duoaa_project\"\n",
    "!export PIP_CACHE_DIR=\"/home/jovyan/groupshare/lukelo_duoaa_project\"\n",
    "!export JUPYTER_RUNTIME_DIR=\"/home/jovyan/groupshare/lukelo_duoaa_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04450533-798e-459b-969f-2986604280a1",
   "metadata": {},
   "source": [
    "## LLM as a JUDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964af22-dd05-45c3-835a-50e2b2ccb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_token')\n",
    "\n",
    "# ======================================================\n",
    "# 1. Judge prompt (with 1–4 scale: Bad…Excellent)\n",
    "# ======================================================\n",
    "\n",
    "JUDGE_SYSTEM = (\n",
    "    \"You are a strict, careful evaluator of autonomous driving scene descriptions. \"\n",
    "    \"You must follow the instructions exactly and output in the required format.\"\n",
    ")\n",
    "\n",
    "JUDGE_USER_TEMPLATE = \"\"\"\n",
    "You are evaluating descriptions of autonomous driving scenes.\n",
    "\n",
    "You are given:\n",
    "- Ground-truth object annotations from the dataset for ONE frame.\n",
    "- A VLM scene description.\n",
    "- A VLM object-focused description.\n",
    "\n",
    "Use ONLY the information in the ground-truth annotations and the two VLM descriptions.\n",
    "Ignore any prior knowledge about driving scenes.\n",
    "\n",
    "Ground-truth annotations (list of objects for this frame):\n",
    "{annotations}\n",
    "\n",
    "VLM scene description:\n",
    "{scene_desc}\n",
    "\n",
    "VLM object description:\n",
    "{object_desc}\n",
    "\n",
    "You must give 3 ratings. For EACH rating, use the following 4-point scale:\n",
    "\n",
    "1 = Bad\n",
    "    - Very poor, mostly wrong or unhelpful\n",
    "2 = Could be Improved\n",
    "    - Some relevant content but many issues or missing pieces\n",
    "3 = Acceptable\n",
    "    - Generally okay and usable, but still clearly improvable\n",
    "4 = Excellent\n",
    "    - Clear, relevant, and strong according to the definition below\n",
    "\n",
    "Apply this scale to the following criteria:\n",
    "\n",
    "1) Hallucinations\n",
    "“Does the VLM description introduce any objects or events that are not present in the ground-truth annotations of this sample?”\n",
    "- 1 (Bad) = many hallucinated objects / events\n",
    "- 2 (Could be Improved) = several hallucinations or serious mistakes\n",
    "- 3 (Acceptable) = a few minor hallucinations\n",
    "- 4 (Excellent) = no obvious hallucinations\n",
    "\n",
    "(Rate this based on BOTH the VLM scene description and the VLM object description.)\n",
    "\n",
    "2) Safety relevance\n",
    "“How well does the description focus on safety-relevant elements: vehicles and pedestrians?”\n",
    "- 1 (Bad) = mostly irrelevant, misses safety-critical stuff\n",
    "- 2 (Could be Improved) = mentions a few relevant things but misses many\n",
    "- 3 (Acceptable) = mentions some important vehicles / pedestrians\n",
    "- 4 (Excellent) = clearly emphasizes safety-critical elements (vehicles and pedestrians)\n",
    "\n",
    "(Rate this based on the VLM object description.)\n",
    "\n",
    "3) Overall quality\n",
    "“Overall, how good is this description as a human explanation of the scene, independent of how factually accurate it is?”\n",
    "- 1 (Bad) = poor, confusing or very unhelpful\n",
    "- 2 (Could be Improved) = partially understandable, but weak overall\n",
    "- 3 (Acceptable) = okay / usable but incomplete\n",
    "- 4 (Excellent) = very good, clear, and easy to understand\n",
    "\n",
    "(Rate this based on the VLM scene description.)\n",
    "\n",
    "IMPORTANT:\n",
    "- Use ONLY integer scores from 1 to 4.\n",
    "- Be consistent with the scale definition.\n",
    "\n",
    "Respond EXACTLY in the following format:\n",
    "\n",
    "Feedback:::\n",
    "Hallucinations: <integer 1-4>\n",
    "Safety relevance: <integer 1-4>\n",
    "Overall quality: <integer 1-4>\n",
    "Short explanation: <one or two sentences explaining your ratings>\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================\n",
    "# 2. Helpers: build prompt & parse output\n",
    "# ======================================================\n",
    "from collections import Counter\n",
    "\n",
    "def extract_category_names(annotations):\n",
    "    \"\"\"\n",
    "    Return a list of unique category_name strings from the annotations.\n",
    "    Example: [\"human.pedestrian.adult\", \"vehicle.car\"]\n",
    "    \"\"\"\n",
    "    if not annotations:\n",
    "        return []\n",
    "\n",
    "    cats = [ann.get(\"category_name\", \"unknown\") for ann in annotations]\n",
    "\n",
    "    # If you don't care about order:\n",
    "    # unique_cats = sorted(set(cats))\n",
    "\n",
    "    # If you want to preserve the order they appear in:\n",
    "    unique_cats = list(dict.fromkeys(cats))\n",
    "\n",
    "    return unique_cats\n",
    "\n",
    "    \n",
    "def build_judge_prompt(sample: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt from one JSON sample.\n",
    "\n",
    "    Expected keys in sample:\n",
    "      - \"annotations\" (list)\n",
    "      - \"vlm_scene_description\" (str)\n",
    "      - \"vlm_object_description\" (str)\n",
    "    \"\"\"\n",
    "    annotations = sample.get(\"annotations\", [])\n",
    "    scene_desc = sample.get(\"vlm_scene_description\", \"\")\n",
    "    object_desc = sample.get(\"vlm_object_description\", \"\")\n",
    "\n",
    "    annotations_text = extract_category_names(annotations)\n",
    "\n",
    "    # annotations_text = json.dumps(annotations, indent=2)\n",
    "\n",
    "    return JUDGE_USER_TEMPLATE.format(\n",
    "        annotations=annotations_text,\n",
    "        scene_desc=scene_desc,\n",
    "        object_desc=object_desc,\n",
    "    )\n",
    "\n",
    "\n",
    "def _extract_int_field(label: str, text: str):\n",
    "    \"\"\"\n",
    "    Extract integer 1–4 after a label like 'Hallucinations:' from the judge output.\n",
    "    \"\"\"\n",
    "    pattern = rf\"{label}\\s*:\\s*([1-4])\"\n",
    "    m = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def parse_judge_output(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse the LLM judge output into numeric scores + explanation.\n",
    "    \"\"\"\n",
    "    hallucinations = _extract_int_field(\"Hallucinations\", text)\n",
    "    safety_relevance = _extract_int_field(\"Safety relevance\", text)\n",
    "    overall_quality = _extract_int_field(\"Overall quality\", text)\n",
    "\n",
    "    m = re.search(r\"Short explanation\\s*:\\s*(.+)\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    explanation = m.group(1).strip() if m else None\n",
    "\n",
    "    return {\n",
    "        \"hallucinations\": hallucinations,\n",
    "        \"safety_relevance\": safety_relevance,\n",
    "        \"overall_quality\": overall_quality,\n",
    "        \"short_explanation\": explanation,\n",
    "        \"raw_judge_output\": text,\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# 3. Call Llama 3 via transformers.pipeline\n",
    "# ======================================================\n",
    "\n",
    "def call_llm_judge_pipeline(\n",
    "    prompt: str,\n",
    "    pipe,\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 300,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use a Llama-3 chat pipeline to get the judge's answer for a single prompt.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # For Llama 3 chat pipeline, generated_text is a list of messages\n",
    "    assistant_msg = outputs[0][\"generated_text\"][-1]\n",
    "    judge_text = assistant_msg[\"content\"]\n",
    "    return judge_text\n",
    "\n",
    "# ======================================================\n",
    "# 4. Main: run_llm_judge over a folder\n",
    "# ======================================================\n",
    "\n",
    "def run_llm_judge(\n",
    "    input_dir: str,\n",
    "    model_id: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    output_csv: str = None,\n",
    "    max_files: int = None,\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run LLM-as-a-judge over all JSON samples in a folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Directory containing per-sample JSON files (e.g., \"JsonFiles/\").\n",
    "    model_id : str\n",
    "        Hugging Face model ID, e.g. \"meta-llama/Meta-Llama-3-8B-Instruct\".\n",
    "    output_csv : str or None\n",
    "        If not None, save the resulting DataFrame to this path.\n",
    "    max_files : int or None\n",
    "        If not None, only process the first `max_files` JSON files.\n",
    "    temperature : float\n",
    "        Sampling temperature for the judge model (0.0 = deterministic).\n",
    "    max_new_tokens : int\n",
    "        Max new tokens for the judge output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        One row per JSON sample with judge scores and raw output.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Collect JSON files\n",
    "    json_paths = sorted(glob.glob(os.path.join(input_dir, \"*.json\")))\n",
    "    if max_files is not None:\n",
    "        json_paths = json_paths[:max_files]\n",
    "\n",
    "    if not json_paths:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found {len(json_paths)} JSON files in {input_dir}\")\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "\n",
    "    # 2) Load Llama 3 pipeline ONCE\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # 3) Loop over files and judge\n",
    "    rows = []\n",
    "    for path in tqdm(json_paths, desc=\"Judging samples\"):\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                sample = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = build_judge_prompt(sample)\n",
    "        print(prompt)\n",
    "    #     # Call LLM judge\n",
    "    #     try:\n",
    "    #         judge_text = call_llm_judge_pipeline(\n",
    "    #             prompt,\n",
    "    #             pipe,\n",
    "    #             temperature=temperature,\n",
    "    #             max_new_tokens=max_new_tokens,\n",
    "    #         )\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error calling model on {path}: {e}\")\n",
    "    #         judge_text = \"\"\n",
    "\n",
    "    #     # Parse scores\n",
    "    #     scores = parse_judge_output(judge_text)\n",
    "\n",
    "    #     row = {\n",
    "    #         \"file\": os.path.basename(path),\n",
    "    #         \"scene_name\": sample.get(\"scene_name\"),\n",
    "    #         \"sample_index\": sample.get(\"sample_index\"),\n",
    "    #         \"hallucinations\": scores[\"hallucinations\"],\n",
    "    #         \"safety_relevance\": scores[\"safety_relevance\"],\n",
    "    #         \"overall_quality\": scores[\"overall_quality\"],\n",
    "    #         \"short_explanation\": scores[\"short_explanation\"],\n",
    "    #         \"raw_judge_output\": scores[\"raw_judge_output\"],\n",
    "    #     }\n",
    "    #     rows.append(row)\n",
    "\n",
    "    # df = pd.DataFrame(rows)\n",
    "\n",
    "    # if output_csv is not None:\n",
    "    #     df.to_csv(output_csv, index=False)\n",
    "    #     print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "    # return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad227db-d010-478d-a042-46f103bb295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_llm_judge(\n",
    "    input_dir=\"/home/jovyan/OpenEMMA/data/JSON_file\",\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    output_csv=\"/home/jovyan/OpenEMMA/data/trial_llm_as_judge\",\n",
    "    max_files=2,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab37e2-9cff-4b0d-9b42-4617ad8317ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_token')\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/groupshare/lukelo_duoaa_project/.cache\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Pipeline device:\", pipeline.device)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a kids chatbot who always responds in kids speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d69f3-91c8-4c68-9a8f-07157726466a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openemma",
   "language": "python",
   "name": "openemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
